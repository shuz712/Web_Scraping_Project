---
title: "reddit"
author: "Shu Zhang"
date: "8/6/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## I. Introduction

This project is about writing a web scraping script in Python.  Since I spend (probably too much) time on Reddit, I decided that it would be the basis for my project.  For the uninitiated, Reddit is a content-aggregator, where users submit text posts or links to thematic subforums (called "subreddits"), and other users vote them up or down and comment on them.  With over 36 million registered users and nearly a million subreddits, there is a lot of content to scrape. <br />

### Methodology
(to be continued..) <br />
I selected 5 subreddits that are my personal favorites---and scraped the post titles, links, date and time of the post, number of votes, the top rated comment on the comment page for that post, commenter's collection of interested subreddits, etc. The five subreddits were: <br />
/r/machinelearning (subreddit for machine learning topics) <br />
/r/statistics () <br />
/r/soccer () <br />
/r/dataisbeautiful () <br />

/r/frugal () <br />


## II. Import Data
```{r packages, message=FALSE}
library(dplyr)
library(ggplot2)
library(wordcloud)
library(chron)
library(ggthemes)
library(scales)
library(colorspace)
library(tm)
library(SnowballC)
library(RColorBrewer)
library(wordcloud2)
```

```{r, cache=TRUE}
reddit = read.csv('reddit.csv',header = TRUE,stringsAsFactors=F)
```

```{r}
dim(reddit)
```
### A. Column Names
16 columns are included in the dataset, among which there are 2 time variables, 4 quantitative, and 9 qualitative features. A detailed data description is shown below:<br /><br />
__date:__  _The date of submission, formatted as ""_<br />
__link:__ _The submissions' link_ <br />
__num_of_comments:__ _Total number of comments posted under each submission_ <br />
__percentage_of_upvotes:__ _num of upvotes/(num of upvotes + downvotes) feedback for each submission_<br />
__source:__ _sumbission's media source_ <br />
__submitter:__ _submission's author name_ <br />
__submitter_link:__ _author's personal posts link_ <br />
__subreddit:__ _category name (Content entries are organized by areas of interest called "subreddits")_ <br />
__time:__ _The time of submission, formatted as ""_<br />
__title:__ _submission's headline_ <br />
__top_comment:__ _the highest rated comment_ <br />
__top_comment_username:__ _reditor's name who gives the top comment_ <br />
__top_comment_vote:__ _The total number of votes(upvotes - downvotes) of top comment_ <br />
__top_comment_child:__ _The total number of child comments below the top comment_ <br />
__topic_vote:__ _the total number of votes(upvotes - downvotes) of the submission_ <br />
__user_interests:__ _A collection of top-comment redditor's interested subreddits （extracted from redditor's personal posts record)_ <br />
```{r}
names(reddit)
```
## III. Data Cleaning
### A. Time & Date
```{r}
date = as.Date(reddit$date, format = "%Y-%m-%d")
```

```{r}

time = chron(times=reddit$time)
```

```{r}
reddit$date <- date
reddit$time <- time
```

```{r, eval=FALSE}
View(reddit)
```

### B. top_comment_vote & topic_vote
Since some of the top_comment_vote are shown as "stickied" when they are 0, we need to define a function to transfer such numbers to regular integers.
```{r}
top_comment_vote <- gsub('stickied','0',reddit$top_comment_vote)
top_comment_vote <- as.integer(top_comment_vote)    
```

```{r}
reddit$top_comment_vote <- top_comment_vote
```
### C. percentage_of_upvotes
Change from strings to numeric values.
```{r}
per <- sapply(reddit$percentage, function(x) as.integer(substr(x,1,nchar(x)-1)))
per = per*0.01
```

```{r}
reddit$percentage_of_upvotes <- per
#unique(reddit$percentage_of_upvotes)
```
### D. user_interests

```{r}
interests = sapply(reddit$user_interests, function(x) substring(x,2,nchar(x)-1))
```

```{r}
interests = sapply(interests, function(x) strsplit(x, split = ', '))
```

```{r}
interests <- sapply(interests, function(x) gsub("'","",x))
```

```{r}
# Define a function for repeating cells
rep_cell <- function(x,y) {
  x1 <- rep(reddit[,x], sapply(y, length))
  return(x1)
}
```

```{r}
reddit2 <- data.frame(top_comment_username=rep_cell("top_comment_username", interests), subreddit = rep_cell("subreddit",interests), interests = unlist(interests))
```

```{r}
head(reddit2,30)
#dim(reddit2)
```

## III. Data Manipulation
### A. Number of Submissions vs Time
```{r}
sub_time <- reddit %>%
  group_by(subreddit, hour=hours(time))%>%
  summarise('total_num_submission'=n()) %>%
  mutate(max=max(total_num_submission))

```

```{r}
sub_time
```
```{r}
sub_time2 <- reddit %>%
  group_by(hour=hours(time)) %>%
  summarise('total_num_submissions'=n())
```

```{r}
sub_time2
```

```{r}
colours <- rainbow_hcl(5, start = 60, end = 250)
ggplot(data=sub_time, aes(x=hour,y=total_num_submission)) +
  geom_bar(aes(fill = subreddit),stat = 'identity',alpha=0.75)+
  theme_igray()+
  scale_fill_manual (values=colours)+
  labs(x="Time of Submission (Hour)", y="Number of Submissions",
       title = "Time vs Number of Reddit Submissions Bar Plot") +
  geom_text(aes(x = hour, y = total_num_submission, label = total_num_submission), 
              size = 1.5, vjust = -5) +
  theme(legend.position="bottom")
  #stat_summary(fun.y=sum, geom="line",  size=0.5, color="sandybrown",alpha=0.75)
  #geom_smooth(alpha=0.05,method='loess',se=FALSE,color="lightsteelblue4",size=0.55)
```
<br />
From the bar plot graph, we can see that the highest volume of submissions occurs between 1:00 p.m. and 8:00 p.m., and peaking at 1:00 pm and 5:00 p.m. The fewest submissions are sent around 5:00 am. The five subreddits share a similar trend characteristics over time throughout a day, regarding to the area plot below.
```{r}
reddit3 <- reddit %>% mutate(hour= hours(time))
```

```{r}
ggplot(data=reddit3,aes(x=hour))+
  geom_density(aes(fill=subreddit),alpha=0.55)+
  #facet_wrap(~subreddit)+
  theme_stata() + #scale_colour_hc("darkunica") +
  labs(x="Time of Submission (Hour)", y="Density of Submissions",
       title = "Time vs Number of Reddit Submissions")
  #theme(legend.position = "none",axis.line = element_line(colour = "black"),panel.border = element_blank())+

```

```{r}
ggplot(data=sub_time,aes(x=hour,y=total_num_submission))+
  geom_area(aes(fill=subreddit,color=subreddit),alpha=0.55,size=0.15,
            stat = "identity")+
  #facet_wrap(~subreddit)+
  theme_igray() + #scale_colour_hc("darkunica") +
  labs(x="Time of Submission (Hour)", y="Number of Submissions",
       title = "Time vs Number of Reddit Submissions")+
  scale_color_hue(c = 50, l = 70, h=c(30, 300))+
  scale_fill_hue(c = 50, l = 70, h=c(30, 300)) +
  theme(legend.position="bottom")
```

### B. Number of votes/comments vs Time
To reduce the effects of large outliers, we narrowed the range of topic_vote down to (0,300) by using the ylim() function. <br / > <br />

We found some popular time slots to get the most of votes to your post: <br />
Note: To minimize the effect of outliers, for example, some hot topics may get extremely high upvotes regardless of a particular time frame, we only take the median of topic_vote into consideration.<br />
_Around 8am_ <br />
_Around noon_ <br />
_Around 8pm_ <br />
_Around 1am_ <br />

We’ve taken the data from all submissions sent through reddit to find the most popular times for posting. Looking at all submissions sent throughout each day, here is an overview of the most popular times to post on reddit: <br />
Around noon, local time, on average for each time zone, is the most popular time to post, in order to get the most number of votes. The three highest volume of voting interactions occurs around 12:00 pm., 8:00 p.m., and 8:00 a.m. The three fewest votes time slots are 2:00 a.m., 6:00 a.m., and interestingly, 3:00 p.m.<br />
```{r}
vote_time <- reddit %>%
  group_by(hour=as.factor(hours(time))) %>%
  select(hour,topic_vote,subreddit) %>%
  mutate(median = median(topic_vote))
```

```{r}
vote_time
```

```{r}
ggplot(data=vote_time,aes(x=hour,y=topic_vote,fill=hour)) +
  geom_boxplot(alpha=0.4)+
  #facet_wrap(~subreddit) +
  ylim(0,300)+
  theme_igray()+
  theme(legend.position = "none")+
  xlab("Hour of a Day")+
  ylab("Number of Votes of Submissions")+
  stat_summary(fun.y=mean, geom="point", shape=20, size=0.5, color="red", fill="red")+
  geom_text(aes(x = hour, y = median, label = median), 
              size = 1.9, vjust = -.5)

    
```

### C.Interests
#### 1.People who like Machine Learning also like:


```{r}
machinelearning <- reddit2 %>% group_by(subreddit) %>% filter(subreddit == "MachineLearning")
```
Top 20 most favourite subreddit for "machinelearning" redditors: <br />
```{r}
ml_interests <- machinelearning %>% group_by(interests) %>% summarise('freq' = n()) %>% mutate('percentage'=round(freq/sum(freq),4)*100) %>%
  arrange(desc(freq)) %>%
  top_n(20)
```

```{r, eval=FALSE}
View(ml_interests)
```

```{r}
colours1 <- rainbow_hcl(20, start = 60, end = 400)
ggplot(ml_interests, aes(reorder(x=interests,freq), y=freq)) +
  geom_bar(stat="identity", aes(fill=interests)) +
  guides(fill=FALSE) +
  coord_flip() +
  xlab("Interested Subjects")+
  ylab("")+
  ggtitle("Most Popular Subreddit for \"machinelearning\" subredditors") +
  #theme_tufte(base_family = "Helvetica") +
  theme_igray() +
  #scale_color_tableau("tableau20")+
  scale_fill_manual(values=colours1)+
  geom_text(data=ml_interests, hjust=-0.5,size=1.5, aes(x=interests,y=freq,label=percentage))
```



#### 2.People who like soccer also like:
```{r}
soccer <- reddit2 %>% group_by(subreddit) %>% filter(subreddit == "soccer")
```
Top 20 most favourite subreddit for "soccer" redditors:
```{r}
sc_interests <- soccer %>% group_by(interests) %>% summarise('freq' = n()) %>% mutate('percentage'=round(freq/sum(freq),4)*100) %>%
  arrange(desc(freq)) %>%
  top_n(20)
```

```{r}
ggplot(sc_interests, aes(reorder(x=interests,freq), y=freq)) +
  geom_bar(stat="identity", aes(fill=interests)) +
  guides(fill=FALSE) +
  coord_flip() +
  xlab("Interested Subjects")+
  ylab("")+
  ggtitle("Most Popular Subreddit for \"soccer\" subredditors") +
  #theme_tufte(base_family = "Helvetica") +
  theme_igray() +
  #scale_color_tableau("tableau20")+
  scale_fill_manual(values=colours1)+
  geom_text(data=sc_interests, hjust=-0.5,size=1.5, aes(x=interests,y=freq,label=percentage))
```


#### 3.People who like frugal also like:
```{r}
frugal <- reddit2 %>% group_by(subreddit) %>% filter(subreddit == "Frugal")
```
Top 15 most favourite subreddit for "frugal" redditors:
```{r}
fg_interests <- frugal %>% group_by(interests) %>% summarise('freq' = n()) %>% mutate('percentage'=round(freq/sum(freq),4)*100) %>%
  arrange(desc(freq)) %>%
  top_n(15)
```

```{r}
ggplot(fg_interests, aes(reorder(x=interests,freq), y=freq)) +
  geom_bar(stat="identity", aes(fill=interests)) +
  guides(fill=FALSE) +
  coord_flip() +
  xlab("Interested Subjects")+
  ylab("")+
  ggtitle("Most Popular Subreddit for \"frugal\" subredditors") +
  #theme_tufte(base_family = "Helvetica") +
  theme_igray() +
  #scale_color_tableau("tableau20")+
  scale_fill_manual(values=colours1)+
  geom_text(data=fg_interests, hjust=-0.5,size=1.5, aes(x=interests,y=freq,label=percentage))
```

#### 4.People who like dataisbeautiful also like:
```{r}
db <- reddit2 %>% group_by(subreddit) %>% filter(subreddit == "dataisbeautiful")
```
Top 20 most favourite subreddit for "dataisbeautiful" redditors:
```{r}
db_interests <- db %>% group_by(interests) %>% summarise('freq' = n()) %>% mutate('percentage'=round(freq/sum(freq),4)*100) %>%
  arrange(desc(freq)) %>%
  top_n(15)
```
```{r}
ggplot(db_interests, aes(reorder(x=interests,freq), y=freq)) +
  geom_bar(stat="identity", aes(fill=interests)) +
  guides(fill=FALSE) +
  coord_flip() +
  xlab("Interested Subjects")+
  ylab("")+
  ggtitle("Most Popular Subreddit for \"dataisbeautiful\" subredditors") +
  #theme_tufte(base_family = "Helvetica") +
  theme_igray() +
  #scale_color_tableau("tableau20")+
  scale_fill_manual(values=colours1)+
  geom_text(data=db_interests, hjust=-0.5,size=1.5, aes(x=interests,y=freq,label=percentage))
```
<br />
#### 5.People who like statistics also like:
```{r}
st <- reddit2 %>% group_by(subreddit) %>% filter(subreddit == "statistics")
```
Top 20 most favourite subreddit for "dataisbeautiful" redditors:
```{r}
st_interests <- st %>% group_by(interests) %>% summarise('freq' = n()) %>% mutate('percentage'=round(freq/sum(freq),4)*100) %>%
  arrange(desc(freq)) %>%
  top_n(15)
```
```{r}
ggplot(st_interests, aes(reorder(x=interests,freq), y=freq)) +
  geom_bar(stat="identity", aes(fill=interests)) +
  guides(fill=FALSE) +
  coord_flip() +
  xlab("Interested Subjects")+
  ylab("")+
  ggtitle("Most Popular Subreddit for \"statistics\" subredditors") +
  #theme_tufte(base_family = "Helvetica") +
  theme_igray() +
  #scale_color_tableau("tableau20")+
  scale_fill_manual(values=colours1)+
  geom_text(data=st_interests, hjust=-0.5,size=1.5, aes(x=interests,y=freq,label=percentage))
```
```{r}
unique(reddit$subreddit)
```


### D.Submission Topics
#### Word Cloud1 

Select top 100 hot topics by their topic_vote in "MachineLearning" subreddit.
```{r}
mlword <- reddit %>%
  group_by(subreddit) %>%
  filter(subreddit == "MachineLearning") %>%
  arrange(desc(topic_vote)) %>%
  select(subreddit,title,topic_vote) 

mlword <- distinct(mlword)

mlword <- mlword[0:100,]


# 228 distinct topic in total, we selected the top 100.
```

##### 1. Load the data as a corpus
```{r}
# Load the data as a corpus
mltext <- mlword$title
mldocs <- Corpus(VectorSource(mltext))
# VectorSource() function creates a corpus of character vectors
```



```{r, eval=FALSE}
# Inspect the content of the document
inspect(mldocs)
```

##### 2. Cleaning the text
```{r}
#Text transformation

#Transformation is performed using tm_map() function to replace, for example, special characters from the text.

#Replacing “/”, “@” and “|” with space:
# Build a function "toSpace" which transfer any pattern to " "
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))


mldocs <- tm_map(mldocs, toSpace, "/")
mldocs <- tm_map(mldocs, toSpace, "@")
mldocs <- tm_map(mldocs, toSpace, "\\|")



# Convert the text to lower case
mldocs <- tm_map(mldocs, content_transformer(tolower))
# Remove numbers
mldocs <- tm_map(mldocs, removeNumbers)
# Remove english common stopwords
mldocs <- tm_map(mldocs, removeWords, stopwords("english"))
# Remove my own stop word
# specify stopwords as a character vector
mldocs <- tm_map(mldocs, removeWords, c("[R]", "[P]","[D]","[N]")) 
# Remove punctuations
mldocs <- tm_map(mldocs, removePunctuation)
# Eliminate extra white spaces
mldocs <- tm_map(mldocs, stripWhitespace)
# Text stemming
mldocs <- tm_map(mldocs, stemDocument)


```
##### 3. Build a term-document matrix
```{r}
#Document matrix is a table containing the frequency of the words. Column names are words and row names are documents. The function TermDocumentMatrix() is from the text mining package .

mlm <- TermDocumentMatrix(mldocs)
m <- as.matrix(mlm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
```

##### 4. Generate the Word cloud
```{r,echo=FALSE,eval=FALSE}
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=100, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```

```{r}
d100 <- d[0:100,]
set.seed(100)
wordcloud2(d100,
          color='random-light',backgroundColor = "grey")
```

#### Word Cloud2 

Select top 100 hot topics by their topic_vote in "soccer" subreddit.
```{r}
scword <- reddit %>%
  group_by(subreddit) %>%
  filter(subreddit == "soccer") %>%
  arrange(desc(topic_vote)) %>%
  select(subreddit,title,topic_vote) 
# length(unique(scword$title)) # 561 distinct titles, we want to select top 100
scword <- distinct(scword)

scword <- scword[0:100,]

```


```{r}
# Load the data as a corpus
sctext <- scword$title
scdocs <- Corpus(VectorSource(sctext))
# VectorSource() function creates a corpus of character vectors
```



```{r, eval=FALSE}
# Inspect the content of the document
inspect(scdocs)
```

##### 2. Cleaning the text
```{r}
#Text transformation

#Transformation is performed using tm_map() function to replace, for example, special characters from the text.

#Replacing “/”, “@” and “|” with space:



scdocs <- tm_map(scdocs, toSpace, "/")
scdocs <- tm_map(scdocs, toSpace, "@")
scdocs <- tm_map(scdocs, toSpace, "\\|")



# Convert the text to lower case
scdocs <- tm_map(scdocs, content_transformer(tolower))
# Remove numbers
scdocs <- tm_map(scdocs, removeNumbers)
# Remove english common stopwords
scdocs <- tm_map(scdocs, removeWords, stopwords("english"))
# Remove my own stop word
# specify stopwords as a character vector
scdocs <- tm_map(scdocs, removeWords, c("[R]", "[P]","[D]","[N]")) 
# Remove punctuations
scdocs <- tm_map(scdocs, removePunctuation)
# Eliminate extra white spaces
scdocs <- tm_map(scdocs, stripWhitespace)
# Text stemming
scdocs <- tm_map(scdocs, stemDocument)


```
##### 3. Build a term-document matrix
```{r}
#Document matrix is a table containing the frequency of the words. Column names are words and row names are documents. The function TermDocumentMatrix() is from the text mining package .

scm <- TermDocumentMatrix(scdocs)
m <- as.matrix(scm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
```

##### 4. Generate the Word cloud
```{r}
d100 <- d[0:100,]
set.seed(11)
wordcloud2(d100,
          color='random-light')
```

